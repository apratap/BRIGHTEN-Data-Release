{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Curate_V2_GPSDerived_Clusters_Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T15:55:00.919219Z",
     "start_time": "2019-01-04T15:54:58.489100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome, Abhishek Pratap!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import datetime as dt\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "import synapseclient\n",
    "syn = synapseclient.Synapse()\n",
    "syn.login()\n",
    "\n",
    "from synapseclient import Activity, File, Schema, Table, as_table_columns\n",
    "from tabulate import tabulate\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly import tools\n",
    "import plotly.graph_objs as go\n",
    "import plotly.figure_factory as figf\n",
    "import plotly.io as pio\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "\n",
    "\n",
    "# get the latest source files from the GSCAP repo\n",
    "try:\n",
    "    t = subprocess.call(['./get_gscap_source.sh']); del t\n",
    "except:\n",
    "    p = subprocess.Popen(\n",
    "        [\"powershell.exe\", \"./get_gscap_source.ps1\"],\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.PIPE)\n",
    "import gps\n",
    "import utils\n",
    "\n",
    "init_notebook_mode()\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T15:28:25.734876Z",
     "start_time": "2019-01-04T15:28:25.513175Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>participant_id</th>\n",
       "      <th>cid</th>\n",
       "      <th>name</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>categories</th>\n",
       "      <th>max_duration</th>\n",
       "      <th>min_duration</th>\n",
       "      <th>mean_duration</th>\n",
       "      <th>std_duration</th>\n",
       "      <th>total_duration</th>\n",
       "      <th>times_entered</th>\n",
       "      <th>mean_ti_between_visits</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EN00590</td>\n",
       "      <td>home</td>\n",
       "      <td>home</td>\n",
       "      <td>41.66319</td>\n",
       "      <td>-87.47607</td>\n",
       "      <td>home</td>\n",
       "      <td>5.683</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.267</td>\n",
       "      <td>2.027</td>\n",
       "      <td>342.322</td>\n",
       "      <td>151</td>\n",
       "      <td>13.417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EN00590</td>\n",
       "      <td>work</td>\n",
       "      <td>work</td>\n",
       "      <td>41.46633</td>\n",
       "      <td>-87.30726</td>\n",
       "      <td>work</td>\n",
       "      <td>7.633</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.151</td>\n",
       "      <td>2.360</td>\n",
       "      <td>77.450</td>\n",
       "      <td>36</td>\n",
       "      <td>56.281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EN00590</td>\n",
       "      <td>x0</td>\n",
       "      <td>not found</td>\n",
       "      <td>41.65877</td>\n",
       "      <td>-87.46267</td>\n",
       "      <td>none</td>\n",
       "      <td>2.117</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.426</td>\n",
       "      <td>0.563</td>\n",
       "      <td>39.167</td>\n",
       "      <td>92</td>\n",
       "      <td>22.078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EN00590</td>\n",
       "      <td>x1</td>\n",
       "      <td>not found</td>\n",
       "      <td>41.66083</td>\n",
       "      <td>-87.46868</td>\n",
       "      <td>none</td>\n",
       "      <td>2.250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.450</td>\n",
       "      <td>2.250</td>\n",
       "      <td>25</td>\n",
       "      <td>70.095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EN00590</td>\n",
       "      <td>x10</td>\n",
       "      <td>not found</td>\n",
       "      <td>41.60260</td>\n",
       "      <td>-87.27194</td>\n",
       "      <td>none</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.017</td>\n",
       "      <td>2</td>\n",
       "      <td>3.708</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  participant_id   cid       name       lat       lon categories  \\\n",
       "0        EN00590  home       home  41.66319 -87.47607       home   \n",
       "1        EN00590  work       work  41.46633 -87.30726       work   \n",
       "2        EN00590    x0  not found  41.65877 -87.46267       none   \n",
       "3        EN00590    x1  not found  41.66083 -87.46868       none   \n",
       "4        EN00590   x10  not found  41.60260 -87.27194       none   \n",
       "\n",
       "   max_duration  min_duration  mean_duration  std_duration  total_duration  \\\n",
       "0         5.683           0.0          2.267         2.027         342.322   \n",
       "1         7.633           0.0          2.151         2.360          77.450   \n",
       "2         2.117           0.0          0.426         0.563          39.167   \n",
       "3         2.250           0.0          0.090         0.450           2.250   \n",
       "4         0.017           0.0          0.008         0.012           0.017   \n",
       "\n",
       "   times_entered  mean_ti_between_visits  \n",
       "0            151                  13.417  \n",
       "1             36                  56.281  \n",
       "2             92                  22.078  \n",
       "3             25                  70.095  \n",
       "4              2                   3.708  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sid = 'syn15667781'\n",
    "df = pd.read_csv(syn.get(sid).path).rename(columns={\n",
    "    'username':'participant_id',\n",
    "})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send through Google/Yelp for context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only want to spend money finding the clusters with semantic context, this excludes the home and work locations. To create a set of those we first split the set of clusters by these values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T15:28:25.744215Z",
     "start_time": "2019-01-04T15:28:25.736918Z"
    }
   },
   "outputs": [],
   "source": [
    "home_and_work = df.loc[\n",
    "    (df.cid == 'home') |\n",
    "    (df.cid == 'work')\n",
    "].copy()\n",
    "\n",
    "df_ = df.loc[\n",
    "    ~((df.cid == 'home') |\n",
    "    (df.cid == 'work'))\n",
    "].copy()\n",
    "assert len(home_and_work) + len(df_) == len(df)\n",
    "\n",
    "del df\n",
    "df = df_\n",
    "del df_\n",
    "\n",
    "print(f'{len(df)} calls will be made per API.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GSCAP scripts require API requests to be in a specific datatype, a PlaceRequest. So, we create a list of PlaceRequests to process. This will change in future versions to not require the specific datatype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T15:28:30.650575Z",
     "start_time": "2019-01-04T15:28:25.746785Z"
    },
    "code_folding": [
     1,
     11
    ]
   },
   "outputs": [],
   "source": [
    "# setup the API requests to Google\n",
    "gmap_requests = [\n",
    "    gps.PlaceRequest(\n",
    "        lat=r.lat, lon=r.lon, radius=50, \n",
    "        source=gps.ApiSource.GMAPS, \n",
    "        rankby=gps.GmapsRankBy.PROMINENCE\n",
    "    )\n",
    "    for r in df.itertuples()\n",
    "]\n",
    "\n",
    "# setup the API requests to Yelp\n",
    "yelp_requests = [\n",
    "    gps.PlaceRequest(\n",
    "        lat=r.lat, lon=r.lon, radius=50, \n",
    "        source=gps.ApiSource.YELP, \n",
    "        rankby=gps.YelpRankBy.BEST_MATCH\n",
    "    )\n",
    "    for r in df.itertuples()\n",
    "]\n",
    "\n",
    "# setup a progress bar\n",
    "pbar, qu = utils.progress_bar(tqdm(total=2*len(df)))\n",
    "\n",
    "# process each series of requests\n",
    "gmap_results = gps.request_nearby_places(gmap_requests, n_jobs=-1, cache_only=True, progress_qu=qu)\n",
    "yelp_results = gps.request_nearby_places(yelp_requests, n_jobs=-1, cache_only=True, progress_qu=qu)\n",
    "\n",
    "# terminate, join and print cache status to maintain API awareness\n",
    "pbar.terminate(); pbar.join(); del pbar, qu\n",
    "print(f'hits: {gmap_results[\"hits\"]}, misses: {gmap_results[\"misses\"]}')\n",
    "print(f'hits: {yelp_results[\"hits\"]}, misses: {yelp_results[\"misses\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine the results into a single data frame for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T15:28:30.663005Z",
     "start_time": "2019-01-04T15:28:30.653162Z"
    }
   },
   "outputs": [],
   "source": [
    "# extract the results\n",
    "gr = gmap_results['request']\n",
    "yr = yelp_results['request']\n",
    "\n",
    "# generate category value counts\n",
    "grvc = gr.major_categories.value_counts()\n",
    "yvc = yr.major_categories.value_counts()\n",
    "\n",
    "# create an index based on the category for outer joins\n",
    "idx = (set(grvc.index).union(set(yvc.index)))\n",
    "\n",
    "# create the data frame and join with the Google results\n",
    "results = pd.DataFrame(index=idx).join(grvc, how='outer')\n",
    "results = results.rename(columns={'major_categories':'Google'})\n",
    "\n",
    "# join with the Yelp results\n",
    "results = results.join(yvc, how='outer')\n",
    "results = results.rename(columns={'major_categories':'Yelp'})\n",
    "\n",
    "# replace all na values with zero\n",
    "results = results.fillna(0).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T15:28:31.830158Z",
     "start_time": "2019-01-04T15:28:30.664883Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(f'Google could not identify {int(results.loc[\"none\"].Google)} of {len(df)} locations')\n",
    "print(f'Yelp could not identify {int(results.loc[\"none\"].Yelp)} of {len(df)} locations')\n",
    "\n",
    "m = np.max([np.max(results.Google), np.max(results.Yelp)])\n",
    "\n",
    "cols = [i for i in results.index if i != 'none']\n",
    "iplot(go.Figure(\n",
    "    [go.Bar(name=c, x=cols, y=results.loc[cols, c]) for c in results.columns],\n",
    "    go.Layout(\n",
    "        title=f'Google Places returns more uniformly distributed results',\n",
    "        yaxis=dict(title='Number of Results'),\n",
    "        xaxis=dict(title='Category')\n",
    "    )\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of note is how tuned Yelp's dataset is for places within the category of dining_out. Google is aware of 'every' type while Yelp is much more focused on 'activities'. For this reason, I chose to prioritize the Google results as they're much more evenly distributed across the categories. Although Google returned many more places, there are still some that Yelp found but Google did not. For those, I take whatever results Yelp provided. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T15:37:02.698708Z",
     "start_time": "2019-01-04T15:37:02.654221Z"
    }
   },
   "outputs": [],
   "source": [
    "# merge and drop columns\n",
    "df_ = gr.merge(yr, on=['lat', 'lon', 'radius'])\\\n",
    "    .drop(columns=['dtRetrieved_x', 'dtRetrieved_y', 'radius'])\n",
    "\n",
    "# create a new column, taking first whatever categories and name Google provided\n",
    "df_['categories'] = df_.major_categories_x\n",
    "df_['name'] = df_.name_x\n",
    "df_['source'] = df_.source_x\n",
    "\n",
    "# for those where Google did not find anything, assign the Yelp results\n",
    "xmask = (df_.name_x == 'not found') | (df_.name_x == 'not found in cache')\n",
    "ymask = (df_.name_y != 'not found') & (df_.name_y != 'not found in cache')\n",
    "\n",
    "df_.loc[xmask & ymask, 'name']       = df_.loc[xmask & ymask, 'name_y']\n",
    "df_.loc[xmask & ymask, 'categories'] = df_.loc[xmask & ymask, 'major_categories_y']\n",
    "df_.loc[xmask & ymask, 'source']     = df_.loc[xmask & ymask, 'source_y']\n",
    "\n",
    "# take only the required columns\n",
    "df_ = df_.reindex(columns=['lat', 'lon', 'name', 'categories'])\n",
    "\n",
    "# # drop any duplicates\n",
    "df_ = df_.loc[[not b for b in df.duplicated(subset=['lat', 'lon'])]]\n",
    "\n",
    "# drop the name and categories columns because we'll rejoin it with the looked up values\n",
    "df = df.drop(columns=['name', 'categories'], errors='ignore')\n",
    "\n",
    "# left join with the main dataframe\n",
    "df = df.merge(df_, on=['lat', 'lon'], how='left')\n",
    "\n",
    "# resort the columns excluding any pii\n",
    "prefix = ['participant_id', 'cid', 'categories']\n",
    "pii = ['lat', 'lon', 'name']\n",
    "to_keep = prefix+sorted(list(set(df.columns)-set(prefix)-set(pii)))\n",
    "df = df.reindex(columns=to_keep)\n",
    "\n",
    "del df_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T15:38:26.719665Z",
     "start_time": "2019-01-04T15:38:26.717329Z"
    }
   },
   "source": [
    "Recombine our results with the home and work clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T15:40:51.747861Z",
     "start_time": "2019-01-04T15:40:51.741057Z"
    }
   },
   "outputs": [],
   "source": [
    "# drop the pii columns\n",
    "home_and_work = home_and_work.reindex(columns=to_keep)\n",
    "\n",
    "# concatenate the dataframes\n",
    "df = pd.concat([home_and_work, df], axis=0).sort_values(by=['participant_id', 'cid'])\n",
    "\n",
    "del home_and_work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set provenance and upload to Synapse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-28T17:59:35.931940Z",
     "start_time": "2018-12-28T17:59:18.156933Z"
    }
   },
   "outputs": [],
   "source": [
    "t = syn.delete(\n",
    "    syn.tableQuery('select * from syn17023313')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T15:51:40.863434Z",
     "start_time": "2019-01-04T15:51:33.858880Z"
    }
   },
   "outputs": [],
   "source": [
    "final = syn.store(Table(\n",
    "    Schema(\n",
    "            name='Passive Cluster Entries Brighten V2',\n",
    "            columns=as_table_columns(df), \n",
    "            parent='syn10848316'),\n",
    "        df\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T15:53:44.440058Z",
     "start_time": "2019-01-04T15:53:43.626042Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sid' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-9bceee3cc809>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Prepare clusters for public release'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mdescription\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Query Google and Yelp APIs for location information, remove PII, and upload as table'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mused\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         executed=[\n\u001b[1;32m      8\u001b[0m             dict(\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sid' is not defined"
     ]
    }
   ],
   "source": [
    "final = syn.setProvenance(\n",
    "    'syn17116695',\n",
    "    activity=Activity(\n",
    "        name='Prepare clusters for public release',\n",
    "        description='Query Google and Yelp APIs for location information, remove PII, and upload as table',\n",
    "        used=[sid],\n",
    "        executed=[\n",
    "            dict(\n",
    "                name='Curate_V2_GPSDerived_Clusters_Data',\n",
    "                url='https://github.com/apratap/BRIGHTEN-Data-Release/blob/master/Curate_V2_GPSDerived_Clusters_Data.ipynb'\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create data dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T15:55:13.184048Z",
     "start_time": "2019-01-04T15:55:13.174152Z"
    }
   },
   "outputs": [],
   "source": [
    "cheat = []\n",
    "for c in df.columns:\n",
    "    cheat.append(\n",
    "        (c, str(df[c].dtype)\\\n",
    "             .replace('object', 'str')\\\n",
    "             .replace('float64', 'float')\\\n",
    "             .replace('int64', 'int')\\\n",
    "             .replace('datetime64[ns]', 'DateTime')\n",
    "        )\n",
    "    )\n",
    "    \n",
    "headers = ['#', 'Variable Name', 'Data Type', 'Description']\n",
    "\n",
    "descriptions = [\n",
    "    'Unique ID',\n",
    "    'Unique ID of participant\\'s identified location',\n",
    "    'Semantic category of location',\n",
    "    'Maximum time in hours spent at location',\n",
    "    'Mean time in hours spent at location',\n",
    "    'Mean time in hours between visits to location',\n",
    "    'Minimum time in hours spent at location',\n",
    "    'Standard deviation of time in hours spent at location',\n",
    "    'Number of times the research participant entered the cluster',\n",
    "    'Total time in hours the research participant spent in the cluster'\n",
    "]\n",
    "\n",
    "cheat = pd.DataFrame(\n",
    "    cheat, \n",
    "    columns=headers[1:-1],\n",
    "    index=list(range(1, len(cheat)+1))\n",
    ")\n",
    "cheat['Description'] = descriptions\n",
    "\n",
    "print(tabulate(\n",
    "    cheat,\n",
    "    headers=headers,\n",
    "    tablefmt='orgtbl'\n",
    ").replace('+', '|'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
