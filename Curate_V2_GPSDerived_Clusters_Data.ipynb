{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Curate_V2_GPSDerived_Clusters_Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T15:55:00.919219Z",
     "start_time": "2019-01-04T15:54:58.489100Z"
    }
   },
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "import synapseclient\n",
    "from synapseclient import Activity, File, Schema, Table, as_table_columns\n",
    "from tabulate import tabulate\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly import tools\n",
    "import plotly.graph_objs as go\n",
    "import plotly.figure_factory as figf\n",
    "import plotly.io as pio\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "\n",
    "\n",
    "# get the latest source files from the GSCAP repo\n",
    "try:\n",
    "    t = subprocess.call(['./get_gscap_source.sh']); del t\n",
    "except:\n",
    "    p = subprocess.Popen(\n",
    "        [\"powershell.exe\", \"./get_gscap_source.ps1\"],\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.PIPE)\n",
    "import gps\n",
    "import utils\n",
    "\n",
    "init_notebook_mode()\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "syn = synapseclient.Synapse()\n",
    "syn.login()\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T15:28:25.734876Z",
     "start_time": "2019-01-04T15:28:25.513175Z"
    }
   },
   "outputs": [],
   "source": [
    "sid = 'syn15667781'\n",
    "df = pd.read_csv(syn.get(sid).path).rename(columns={\n",
    "    'username':'participant_id',\n",
    "})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send through Google/Yelp for context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only want to spend money finding the clusters with semantic context, this excludes the home and work locations. To create a set of those we first split the set of clusters by these values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T15:28:25.744215Z",
     "start_time": "2019-01-04T15:28:25.736918Z"
    }
   },
   "outputs": [],
   "source": [
    "home_and_work = df.loc[\n",
    "    (df.cid == 'home') |\n",
    "    (df.cid == 'work')\n",
    "].copy()\n",
    "\n",
    "df_ = df.loc[\n",
    "    ~((df.cid == 'home') |\n",
    "    (df.cid == 'work'))\n",
    "].copy()\n",
    "assert len(home_and_work) + len(df_) == len(df)\n",
    "\n",
    "del df\n",
    "df = df_\n",
    "del df_\n",
    "\n",
    "print(f'{len(df)} calls will be made per API.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GSCAP scripts require API requests to be in a specific datatype, a PlaceRequest. So, we create a list of PlaceRequests to process. This will change in future versions to not require the specific datatype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T15:28:30.650575Z",
     "start_time": "2019-01-04T15:28:25.746785Z"
    },
    "code_folding": [
     1,
     11
    ]
   },
   "outputs": [],
   "source": [
    "# setup the API requests to Google\n",
    "gmap_requests = [\n",
    "    gps.PlaceRequest(\n",
    "        lat=r.lat, lon=r.lon, radius=50, \n",
    "        source=gps.ApiSource.GMAPS, \n",
    "        rankby=gps.GmapsRankBy.PROMINENCE\n",
    "    )\n",
    "    for r in df.itertuples()\n",
    "]\n",
    "\n",
    "# setup the API requests to Yelp\n",
    "yelp_requests = [\n",
    "    gps.PlaceRequest(\n",
    "        lat=r.lat, lon=r.lon, radius=50, \n",
    "        source=gps.ApiSource.YELP, \n",
    "        rankby=gps.YelpRankBy.BEST_MATCH\n",
    "    )\n",
    "    for r in df.itertuples()\n",
    "]\n",
    "\n",
    "# setup a progress bar\n",
    "pbar, qu = utils.progress_bar(tqdm(total=2*len(df)))\n",
    "\n",
    "# process each series of requests\n",
    "gmap_results = gps.request_nearby_places(gmap_requests, n_jobs=-1, cache_only=True, progress_qu=qu)\n",
    "yelp_results = gps.request_nearby_places(yelp_requests, n_jobs=-1, cache_only=True, progress_qu=qu)\n",
    "\n",
    "# terminate, join and print cache status to maintain API awareness\n",
    "pbar.terminate(); pbar.join(); del pbar, qu\n",
    "print(f'hits: {gmap_results[\"hits\"]}, misses: {gmap_results[\"misses\"]}')\n",
    "print(f'hits: {yelp_results[\"hits\"]}, misses: {yelp_results[\"misses\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine the results into a single data frame for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T15:28:30.663005Z",
     "start_time": "2019-01-04T15:28:30.653162Z"
    }
   },
   "outputs": [],
   "source": [
    "# extract the results\n",
    "gr = gmap_results['request']\n",
    "yr = yelp_results['request']\n",
    "\n",
    "# generate category value counts\n",
    "grvc = gr.major_categories.value_counts()\n",
    "yvc = yr.major_categories.value_counts()\n",
    "\n",
    "# create an index based on the category for outer joins\n",
    "idx = (set(grvc.index).union(set(yvc.index)))\n",
    "\n",
    "# create the data frame and join with the Google results\n",
    "results = pd.DataFrame(index=idx).join(grvc, how='outer')\n",
    "results = results.rename(columns={'major_categories':'Google'})\n",
    "\n",
    "# join with the Yelp results\n",
    "results = results.join(yvc, how='outer')\n",
    "results = results.rename(columns={'major_categories':'Yelp'})\n",
    "\n",
    "# replace all na values with zero\n",
    "results = results.fillna(0).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T15:28:31.830158Z",
     "start_time": "2019-01-04T15:28:30.664883Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(f'Google could not identify {int(results.loc[\"none\"].Google)} of {len(df)} locations')\n",
    "print(f'Yelp could not identify {int(results.loc[\"none\"].Yelp)} of {len(df)} locations')\n",
    "\n",
    "m = np.max([np.max(results.Google), np.max(results.Yelp)])\n",
    "\n",
    "cols = [i for i in results.index if i != 'none']\n",
    "iplot(go.Figure(\n",
    "    [go.Bar(name=c, x=cols, y=results.loc[cols, c]) for c in results.columns],\n",
    "    go.Layout(\n",
    "        title=f'Google Places returns more uniformly distributed results',\n",
    "        yaxis=dict(title='Number of Results'),\n",
    "        xaxis=dict(title='Category')\n",
    "    )\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of note is how tuned Yelp's dataset is for places within the category of dining_out. Google is aware of 'every' type while Yelp is much more focused on 'activities'. For this reason, I chose to prioritize the Google results as they're much more evenly distributed across the categories. Although Google returned many more places, there are still some that Yelp found but Google did not. For those, I take whatever results Yelp provided. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T15:37:02.698708Z",
     "start_time": "2019-01-04T15:37:02.654221Z"
    }
   },
   "outputs": [],
   "source": [
    "# merge and drop columns\n",
    "df_ = gr.merge(yr, on=['lat', 'lon', 'radius'])\\\n",
    "    .drop(columns=['dtRetrieved_x', 'dtRetrieved_y', 'radius'])\n",
    "\n",
    "# create a new column, taking first whatever categories and name Google provided\n",
    "df_['categories'] = df_.major_categories_x\n",
    "df_['name'] = df_.name_x\n",
    "df_['source'] = df_.source_x\n",
    "\n",
    "# for those where Google did not find anything, assign the Yelp results\n",
    "xmask = (df_.name_x == 'not found') | (df_.name_x == 'not found in cache')\n",
    "ymask = (df_.name_y != 'not found') & (df_.name_y != 'not found in cache')\n",
    "\n",
    "df_.loc[xmask & ymask, 'name']       = df_.loc[xmask & ymask, 'name_y']\n",
    "df_.loc[xmask & ymask, 'categories'] = df_.loc[xmask & ymask, 'major_categories_y']\n",
    "df_.loc[xmask & ymask, 'source']     = df_.loc[xmask & ymask, 'source_y']\n",
    "\n",
    "# take only the required columns\n",
    "df_ = df_.reindex(columns=['lat', 'lon', 'name', 'categories'])\n",
    "\n",
    "# # drop any duplicates\n",
    "df_ = df_.loc[[not b for b in df.duplicated(subset=['lat', 'lon'])]]\n",
    "\n",
    "# drop the name and categories columns because we'll rejoin it with the looked up values\n",
    "df = df.drop(columns=['name', 'categories'], errors='ignore')\n",
    "\n",
    "# left join with the main dataframe\n",
    "df = df.merge(df_, on=['lat', 'lon'], how='left')\n",
    "\n",
    "# resort the columns excluding any pii\n",
    "prefix = ['participant_id', 'cid', 'categories']\n",
    "pii = ['lat', 'lon', 'name']\n",
    "to_keep = prefix+sorted(list(set(df.columns)-set(prefix)-set(pii)))\n",
    "df = df.reindex(columns=to_keep)\n",
    "\n",
    "del df_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T15:38:26.719665Z",
     "start_time": "2019-01-04T15:38:26.717329Z"
    }
   },
   "source": [
    "Recombine our results with the home and work clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T15:40:51.747861Z",
     "start_time": "2019-01-04T15:40:51.741057Z"
    }
   },
   "outputs": [],
   "source": [
    "# drop the pii columns\n",
    "home_and_work = home_and_work.reindex(columns=to_keep)\n",
    "\n",
    "# concatenate the dataframes\n",
    "df = pd.concat([home_and_work, df], axis=0).sort_values(by=['participant_id', 'cid'])\n",
    "\n",
    "del home_and_work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set provenance and upload to Synapse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-28T17:59:35.931940Z",
     "start_time": "2018-12-28T17:59:18.156933Z"
    }
   },
   "outputs": [],
   "source": [
    "t = syn.delete(\n",
    "    syn.tableQuery('select * from syn17023313')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T15:51:40.863434Z",
     "start_time": "2019-01-04T15:51:33.858880Z"
    }
   },
   "outputs": [],
   "source": [
    "final = syn.store(Table(\n",
    "    Schema(\n",
    "            name='GPS Identified Clusters (V2)',\n",
    "            columns=as_table_columns(df), \n",
    "            parent='syn10848316'),\n",
    "        df\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T15:53:44.440058Z",
     "start_time": "2019-01-04T15:53:43.626042Z"
    }
   },
   "outputs": [],
   "source": [
    "final = syn.setProvenance(\n",
    "    'syn17115518',\n",
    "    activity=Activity(\n",
    "        name='Prepare clusters for public release',\n",
    "        description='Query Google and Yelp APIs for location information, remove PII, and upload as table',\n",
    "        used=[sid],\n",
    "        executed=[\n",
    "            dict(\n",
    "                name='IPython Notebook',\n",
    "                url='https://github.com/apratap/BRIGHTEN-Data-Release/blob/master/Create_V2GPSClusters_datafiles.ipynb'\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create data dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T15:55:13.184048Z",
     "start_time": "2019-01-04T15:55:13.174152Z"
    }
   },
   "outputs": [],
   "source": [
    "cheat = []\n",
    "for c in df.columns:\n",
    "    cheat.append(\n",
    "        (c, str(df[c].dtype)\\\n",
    "             .replace('object', 'str')\\\n",
    "             .replace('float64', 'float')\\\n",
    "             .replace('int64', 'int')\\\n",
    "             .replace('datetime64[ns]', 'DateTime')\n",
    "        )\n",
    "    )\n",
    "    \n",
    "headers = ['#', 'Variable Name', 'Data Type', 'Description']\n",
    "\n",
    "descriptions = [\n",
    "    'Unique ID',\n",
    "    'Unique ID of participant\\'s identified location',\n",
    "    'Semantic category of location',\n",
    "    'Maximum time in hours spent at location',\n",
    "    'Mean time in hours spent at location',\n",
    "    'Mean time in hours between visits to location',\n",
    "    'Minimum time in hours spent at location',\n",
    "    'Standard deviation of time in hours spent at location',\n",
    "    'Number of times the research participant entered the cluster',\n",
    "    'Total time in hours the research participant spent in the cluster'\n",
    "]\n",
    "\n",
    "cheat = pd.DataFrame(\n",
    "    cheat, \n",
    "    columns=headers[1:-1],\n",
    "    index=list(range(1, len(cheat)+1))\n",
    ")\n",
    "cheat['Description'] = descriptions\n",
    "\n",
    "print(tabulate(\n",
    "    cheat,\n",
    "    headers=headers,\n",
    "    tablefmt='orgtbl'\n",
    ").replace('+', '|'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
